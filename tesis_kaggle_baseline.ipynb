{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tckdzztVFT0dQryV0gnYu3wmSAKlL6xR",
      "authorship_tag": "ABX9TyM0GnjD/mtF/s0D8618WRSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuisVenator/AlteredCoal_1.15/blob/master/tesis_kaggle_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Prediction from Kaggle Dataset"
      ],
      "metadata": {
        "id": "zQsL3rBBw1X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup to allow running locally\n",
        "We sometimes may want to run this notebook locally or in a managed colab runtime. Here we set it up in a way that allows us to do both. If we set it up on the host such that the local folder also syncs to drive, we can even go back and forth between running locally and remotely. This needs the local runtime to have a folder with the same layout as in the drive.\n",
        "To run the docker image then:\n",
        "\n",
        "`docker run --mount type=bind,src=\"G:/tesis\",target=/mnt/tesis -p 127.0.0.1:9000:8080 us-docker.pkg.dev/colab-images/public/runtime`"
      ],
      "metadata": {
        "id": "7CFMHNSwwZBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  IN_COLAB = True\n",
        "  DRIVE_PATH = '/content/drive/Othercomputers/My Computer/tesis'\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  DRIVE_PATH = '/mnt/tesis'\n",
        "\n",
        "print(IN_COLAB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQXgMNElxGOc",
        "outputId": "1b95ecca-e72d-4b3c-a4a6-75eebd5210ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First we load datasets from drive and display them"
      ],
      "metadata": {
        "id": "4mrUslDvw8xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import holidays\n",
        "import numpy as np\n",
        "from IPython.display import Markdown, display\n",
        "import pickle\n",
        "import lightgbm\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "def read_data(debug_print=False):\n",
        "    data: dict[str, pd.DataFrame] = {}\n",
        "    data['client'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/client.csv')\n",
        "    data['electricity_prices'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/electricity_prices.csv')\n",
        "    data['forecast_weather'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/forecast_weather.csv')\n",
        "    data['gas_prices'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/gas_prices.csv')\n",
        "    data['train'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/train.csv')\n",
        "    data['historical_weather'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/historical_weather.csv')\n",
        "    data['weather_station_to_county_mapping'] = pd.read_csv(f'{DRIVE_PATH}/datasets/kaggle/weather_station_to_county_mapping.csv')\n",
        "\n",
        "    if debug_print:\n",
        "        for name, df in data.items():\n",
        "            print(f'{name}: {df.shape}')\n",
        "            display(df.head())\n",
        "\n",
        "    return data['train'], data['client'], data[\"historical_weather\"], data[\"forecast_weather\"], data[\"gas_prices\"], data[\"electricity_prices\"], data[\"weather_station_to_county_mapping\"]\n"
      ],
      "metadata": {
        "id": "US5cPVzmvY_q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "The data we have has a nice readable format for humans, but not computers. For example the dates need to be transformed to cyclical data so the model can understand the seasonality. If we just directly encode months to numbers, december will be seen as very different from january.\n",
        "\n",
        "Also some data between tables needs to be unified. For the weather forecast gives us latitude and longitude, but our prediction run on counties, so we need to translate positions to counties. Another example is wind direction as degrees in historical weather and as vector in forecast.\n",
        "\n",
        "Lastly we want to add some features. This includes lagged data points. It is evident that on consecutive days (without other mayor changes between them) data from the first day is a pretty good indicator for behavior on the second day. Another data point we add is wether or not a given day is a holiday or weekend, as data analysis has shown first that weekends show markably different behavior than weekdays and also that holidays behave much more like the former than the later."
      ],
      "metadata": {
        "id": "BkkhtjFYM9Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_weather_map(df_map):\n",
        "    df_map = df_map.dropna().copy()\n",
        "    df_map['latitude'] = df_map['latitude'].astype(float).round(1)\n",
        "    df_map['longitude'] = df_map['longitude'].astype(float).round(1)\n",
        "    return df_map\n",
        "\n",
        "def forecast_map(df, df_map):\n",
        "    df['latitude'] = df['latitude'].astype(float).round(1)\n",
        "    df['longitude'] = df['longitude'].astype(float).round(1)\n",
        "\n",
        "    df = df.merge(df_map[['latitude', 'longitude', 'county']], how='inner', on=['latitude', 'longitude'])\n",
        "    df = df.dropna(subset=['county'])\n",
        "\n",
        "    df.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
        "    df.drop(columns=['origin_datetime', 'latitude', 'longitude'], inplace=True)\n",
        "\n",
        "    df = df.sort_values(by='hours_ahead')\n",
        "    df = df.groupby(['county', 'datetime', 'hours_ahead', 'data_block_id']).mean().reset_index()\n",
        "    df = df.groupby(['county', 'datetime', 'data_block_id']).first().reset_index()\n",
        "\n",
        "    return df.add_suffix('_forecast') \\\n",
        "             .rename(columns={'county_forecast': 'county', 'datetime_forecast': 'datetime'})\n",
        "\n",
        "\n",
        "def historical_map(df, df_map):\n",
        "    df['latitude'] = df['latitude'].astype(float).round(1)\n",
        "    df['longitude'] = df['longitude'].astype(float).round(1)\n",
        "\n",
        "    df = df.merge(df_map[['latitude', 'longitude', 'county']], how='inner', on=['latitude', 'longitude'])\n",
        "    df = df.dropna(subset=['county'])\n",
        "    df.drop(columns=['latitude', 'longitude'], inplace=True)\n",
        "\n",
        "    # CHECKIF: maybe max or min improves?\n",
        "    return df.groupby(['county', 'datetime']).mean().reset_index().add_suffix('_historical') \\\n",
        "           .rename(columns={'county_historical': 'county'})\n",
        "\n",
        "\n",
        "def merge_data(df_train, df_client, df_historical_weather, df_forecast_weather, df_gas_prices, df_electricity_prices, df_weather_station_to_county_mapping):\n",
        "    # Convert coordinates to county codes\n",
        "    df_weather_station_to_county_mapping = preprocess_weather_map(df_weather_station_to_county_mapping)\n",
        "    df_historical_weather = historical_map(df_historical_weather, df_weather_station_to_county_mapping).rename(columns={'data_block_id_historical': 'data_block_id'})\n",
        "    df_forecast_weather = forecast_map(df_forecast_weather, df_weather_station_to_county_mapping).rename(columns={'data_block_id_forecast': 'data_block_id'})\n",
        "\n",
        "\n",
        "    # Fix date and time for pandas\n",
        "    df_train['datetime'] = pd.to_datetime(df_train['datetime'])\n",
        "    df_train['time'] = df_train['datetime'].dt.time\n",
        "    df_historical_weather['datetime_historical'] = pd.to_datetime(df_historical_weather['datetime_historical'])\n",
        "    df_historical_weather['time'] = df_historical_weather['datetime_historical'].dt.time\n",
        "    df_forecast_weather['datetime'] = pd.to_datetime(df_forecast_weather['datetime'])\n",
        "    df_forecast_weather['datetime'] = df_forecast_weather['datetime'].dt.tz_localize('UTC').dt.tz_convert('Europe/Tallinn').dt.tz_localize(None)\n",
        "    ## We need to treat the days with time change somehow, so we just drop duplicated hour\n",
        "    df_forecast_weather = df_forecast_weather.drop_duplicates(subset=['data_block_id', 'datetime', 'county'])\n",
        "\n",
        "    df_gas_prices.drop(columns=['origin_date', 'forecast_date'], inplace=True)\n",
        "\n",
        "    df_electricity_prices['time'] = pd.to_datetime(df_electricity_prices['forecast_date']).dt.time\n",
        "    df_electricity_prices.drop(columns=['origin_date', 'forecast_date'], inplace=True)\n",
        "\n",
        "    # Merge client data\n",
        "    df_client.drop(columns=['date'], inplace=True)\n",
        "    df_client = df_client.groupby(['is_business', 'county', 'product_type', 'data_block_id']).sum().reset_index()\n",
        "    df_train = pd.merge(df_train, df_client, on=['is_business', 'county', 'product_type', 'data_block_id'], how=\"left\")\n",
        "\n",
        "    # Merge weather data\n",
        "    df_train = pd.merge(df_train, df_historical_weather, on=['data_block_id', 'time', 'county'], how='left')\n",
        "    df_train = pd.merge(df_train, df_forecast_weather, on=['data_block_id', 'datetime', 'county'], how='left')\n",
        "    # Now go through and replace NaN for counties without weather data with the mean\n",
        "    weather_nan_cols = df_train.columns[df_train.isna().any()].tolist()\n",
        "    mean_weather = df_train.groupby('datetime')[weather_nan_cols].transform('mean')\n",
        "    df_train[weather_nan_cols] = df_train[weather_nan_cols].fillna(mean_weather)\n",
        "\n",
        "    # Merge price data\n",
        "    df_train = pd.merge(df_train, df_gas_prices, on=['data_block_id'], how='left')\n",
        "    df_train = pd.merge(df_train, df_electricity_prices, on=['data_block_id', 'time'], how='left')\n",
        "\n",
        "\n",
        "    return df_train\n",
        "\n",
        "def training_specific_merge(df_train):\n",
        "    new_columns =  []\n",
        "    for timedelta in range(2,8):\n",
        "        df_prev_target = df_train[['datetime', 'county', 'target', 'is_consumption', 'is_business', 'product_type']].copy()\n",
        "        df_prev_target['datetime'] += pd.Timedelta(days=timedelta)\n",
        "        df_prev_target = df_prev_target.rename(columns={'target': f'target_{timedelta}_ago'})\n",
        "        new_columns.append(f'target_{timedelta}_ago')\n",
        "\n",
        "        df_train = pd.merge(df_train, df_prev_target, on=['datetime', 'county', 'is_consumption', 'is_business', 'product_type'], how='left')\n",
        "\n",
        "    df_train['mean_previous_targets'] = df_train[new_columns].mean(axis=1)\n",
        "\n",
        "    # Add target over capacity\n",
        "    df_train['relative_target'] = df_train['target'] / df_train['installed_capacity']\n",
        "\n",
        "    return df_train\n",
        "\n",
        "def add_prev_target(df, revealed_targets):\n",
        "    new_columns =  []\n",
        "    for timedelta in range(2,8):\n",
        "        df_prev_target = revealed_targets[['datetime', 'county', 'target', 'is_consumption', 'is_business', 'product_type']].copy()\n",
        "        df_prev_target['datetime'] += pd.Timedelta(days=timedelta)\n",
        "        df_prev_target = df_prev_target.rename(columns={'target': f'target_{timedelta}_ago'})\n",
        "        new_columns.append(f'target_{timedelta}_ago')\n",
        "\n",
        "        df = pd.merge(df, df_prev_target, on=['datetime', 'county', 'is_consumption', 'is_business', 'product_type'], how='left')\n",
        "\n",
        "    df['mean_previous_targets'] = df[new_columns].mean(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def transform_data(df):\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['weekday'] = df['datetime'].dt.day_of_week\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "    # Transform cyclical time data\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 11)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 11)\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 23)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 23)\n",
        "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 6)\n",
        "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 6)\n",
        "\n",
        "    ee_holidays = holidays.EE()\n",
        "    df['is_holiday'] = [1 if date in ee_holidays else 0 for date in df['datetime']]\n",
        "\n",
        "    df['wind_direction_radians'] = np.radians(df['winddirection_10m_historical'])\n",
        "    df['10_metre_u_wind_component_historical'] = np.cos(df['wind_direction_radians'])\n",
        "    df['10_metre_v_wind_component_historical'] = np.sin(df['wind_direction_radians'])\n",
        "    df = df.drop(columns=['winddirection_10m_historical', 'wind_direction_radians'])\n",
        "\n",
        "    df = standarize(df, ['target_2_ago', 'target_3_ago', 'target_4_ago', 'target_5_ago', 'target_6_ago', 'target_7_ago', 'surface_solar_radiation_downwards_forecast'])\n",
        "    df = onehot(df, ['county', 'product_type'])\n",
        "\n",
        "    return df\n",
        "\n",
        "standarize_means = {}\n",
        "standarize_std = {}\n",
        "def standarize(df, columns):\n",
        "    for col in columns:\n",
        "        col_mean = df[col].mean()\n",
        "        col_std = df[col].std()\n",
        "        standarize_means[col] = col_mean\n",
        "        standarize_std[col] = col_std\n",
        "        if col_std != 0:\n",
        "            df.loc[:, col] = df[col].apply(lambda x: (x - col_mean) / col_std)\n",
        "        else:\n",
        "            df.loc[:, col] = df[col].apply(lambda x: 0)\n",
        "    return df\n",
        "\n",
        "def onehot(df, columns):\n",
        "    for column in columns:\n",
        "        one_hot = pd.get_dummies(df[column], prefix=column)\n",
        "        df = pd.concat([df, one_hot], axis=1)\n",
        "    df = df.drop(columns=columns)\n",
        "    return df\n",
        "\n",
        "# Debugging only function\n",
        "def check_duplicates(df, stage):\n",
        "    duplicates = df.duplicated('row_id', keep=False)\n",
        "    if duplicates.any():\n",
        "        dup_rows = df[duplicates]\n",
        "        sorted_dup = dup_rows.sort_values(by='row_id')\n",
        "        display(sorted_dup)\n",
        "        raise ValueError(f\"row_id contains duplicates in stage {stage}!\")"
      ],
      "metadata": {
        "id": "v2Oe__mfPgqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining features for production and consumption\n",
        "We decide to separate production and consumption predictions. The reason for this is that data analysis shows that some features can lead to improvements in one but not the other. Probably the easiest example that makes intuitive sense are lagged targets. For production in most cases we would assume that it is the same on days with identical weather conditions. For consumption on the other hand we would assume that there are factors that can not be captured by the features that will affect the consumption, even on days with exactly equal weather conditions. A possible scenario could be a holiday season, when maybe people visit more a specific county. If we include lagged consumption data and see that for the past n days the consumption was very high or low, it is reasonably to assume that this behavior persists until proven otherwise."
      ],
      "metadata": {
        "id": "-sQVawEuQ9kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_con = [\n",
        "    'target_2_ago',\n",
        "    'target_3_ago',\n",
        "    'target_4_ago',\n",
        "    'target_5_ago',\n",
        "    'target_6_ago',\n",
        "    'target_7_ago',\n",
        "    'temperature_forecast',\n",
        "    'dewpoint_forecast',\n",
        "    'cloudcover_high_forecast',\n",
        "    'cloudcover_mid_forecast',\n",
        "    'cloudcover_low_forecast',\n",
        "    'cloudcover_total_forecast',\n",
        "    '10_metre_u_wind_component_forecast',\n",
        "    '10_metre_v_wind_component_forecast',\n",
        "    'direct_solar_radiation_forecast',\n",
        "    'surface_solar_radiation_downwards_forecast',\n",
        "    'temperature_historical',\n",
        "    'dewpoint_historical',\n",
        "    'rain_historical',\n",
        "    'snowfall_historical',\n",
        "    'surface_pressure_historical',\n",
        "    'cloudcover_high_historical',\n",
        "    'cloudcover_mid_historical',\n",
        "    'cloudcover_low_historical',\n",
        "    'cloudcover_total_historical',\n",
        "    '10_metre_u_wind_component_historical',\n",
        "    '10_metre_v_wind_component_historical',\n",
        "    'shortwave_radiation_historical',\n",
        "    'direct_solar_radiation_historical',\n",
        "    'diffuse_radiation_historical',\n",
        "    'lowest_price_per_mwh',\n",
        "    'highest_price_per_mwh',\n",
        "    'euros_per_mwh',\n",
        "    'eic_count',\n",
        "    'installed_capacity',\n",
        "    'product_type_0',\n",
        "    'product_type_1',\n",
        "    'product_type_2',\n",
        "    'product_type_3',\n",
        "    'is_business',\n",
        "    'county_0',\n",
        "    'county_1',\n",
        "    'county_2',\n",
        "    'county_3',\n",
        "    'county_4',\n",
        "    'county_5',\n",
        "    'county_6',\n",
        "    'county_7',\n",
        "    'county_8',\n",
        "    'county_9',\n",
        "    'county_10',\n",
        "    'county_11',\n",
        "    'county_12',\n",
        "    'county_13',\n",
        "    'county_14',\n",
        "    'county_15',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'weekday_sin',\n",
        "    'weekday_cos',\n",
        "    'hour_sin',\n",
        "    'hour_cos',\n",
        "    'is_holiday',\n",
        "    'mean_previous_targets'\n",
        "]\n",
        "\n",
        "features_prod = [\n",
        "    'target_2_ago',\n",
        "    'target_3_ago',\n",
        "    'target_4_ago',\n",
        "    'target_5_ago',\n",
        "    'target_6_ago',\n",
        "    'target_7_ago',\n",
        "    'temperature_forecast',\n",
        "    'dewpoint_forecast',\n",
        "    'cloudcover_high_forecast',\n",
        "    'cloudcover_mid_forecast',\n",
        "    'cloudcover_low_forecast',\n",
        "    'cloudcover_total_forecast',\n",
        "    '10_metre_u_wind_component_forecast',\n",
        "    '10_metre_v_wind_component_forecast',\n",
        "    'direct_solar_radiation_forecast',\n",
        "    'surface_solar_radiation_downwards_forecast',\n",
        "    'temperature_historical',\n",
        "    'dewpoint_historical',\n",
        "    'rain_historical',\n",
        "    'snowfall_historical',\n",
        "    'surface_pressure_historical',\n",
        "    'cloudcover_high_historical',\n",
        "    'cloudcover_mid_historical',\n",
        "    'cloudcover_low_historical',\n",
        "    'cloudcover_total_historical',\n",
        "    '10_metre_u_wind_component_historical',\n",
        "    '10_metre_v_wind_component_historical',\n",
        "    'shortwave_radiation_historical',\n",
        "    'direct_solar_radiation_historical',\n",
        "    'diffuse_radiation_historical',\n",
        "    'lowest_price_per_mwh',\n",
        "    'highest_price_per_mwh',\n",
        "    'euros_per_mwh',\n",
        "    'eic_count',\n",
        "    'installed_capacity',\n",
        "    'product_type_0',\n",
        "    'product_type_1',\n",
        "    'product_type_2',\n",
        "    'product_type_3',\n",
        "    'is_business',\n",
        "    'county_0',\n",
        "    'county_1',\n",
        "    'county_2',\n",
        "    'county_3',\n",
        "    'county_4',\n",
        "    'county_5',\n",
        "    'county_6',\n",
        "    'county_7',\n",
        "    'county_8',\n",
        "    'county_9',\n",
        "    'county_10',\n",
        "    'county_11',\n",
        "    'county_12',\n",
        "    'county_13',\n",
        "    'county_14',\n",
        "    'county_15',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'weekday_sin',\n",
        "    'weekday_cos',\n",
        "    'hour_sin',\n",
        "    'hour_cos',\n",
        "    'is_holiday'\n",
        "]"
      ],
      "metadata": {
        "id": "tMyddCixSO_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train function\n",
        "Lastly we define the function that will make our predictions. The parameters for our model here are already result of hyperparameter optimizations that have been run separately."
      ],
      "metadata": {
        "id": "JlTs2O_nS-vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    df_train, df_client, df_historical_weather, df_forecast_weather, df_gas_prices, df_electricity_prices, df_weather_station_to_county_mapping = read_data()\n",
        "\n",
        "#     df_train = df_train[df_train[\"datetime\"] > '2021-12-31']\n",
        "\n",
        "    df = merge_data(df_train.copy(), df_client, df_historical_weather, df_forecast_weather, df_gas_prices, df_electricity_prices, df_weather_station_to_county_mapping)\n",
        "    df = training_specific_merge(df)\n",
        "    df = transform_data(df).dropna()\n",
        "\n",
        "    df_consumption = df[df['is_consumption'] == 1]\n",
        "    df_production = df[df['is_consumption'] == 0]\n",
        "\n",
        "    models = {\n",
        "        \"consumption\": {\n",
        "            \"data\": df_consumption,\n",
        "            \"models\": [],\n",
        "            \"mae\": 0,\n",
        "            # From 5 fold cross validation (grouped by data_block_id), 100 iteration random search parameter optimization. MAE: 55.84955665117576\n",
        "            \"params\": [\n",
        "                {\n",
        "                    \"num_leaves\": 49,\n",
        "                    \"min_child_samples\": 87,\n",
        "                    \"min_child_weight\": 0.02057785716550018,\n",
        "                    \"subsample\": 0.794696861183782,\n",
        "                    \"colsample_bytree\": 0.9624395150874216,\n",
        "                    \"reg_alpha\": 0.8687887310208573,\n",
        "                    \"reg_lambda\": 0.7001568153893514,\n",
        "                    \"learning_rate\": 0.139020672406113,\n",
        "                    \"n_estimators\": 794\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 47,\n",
        "                    \"min_child_samples\": 237,\n",
        "                    \"min_child_weight\": 0.008327236865873834,\n",
        "                    \"subsample\": 0.7824279936868144,\n",
        "                    \"colsample_bytree\": 0.9140703845572055,\n",
        "                    \"reg_alpha\": 0.39934756431671947,\n",
        "                    \"reg_lambda\": 1.0284688768272232,\n",
        "                    \"learning_rate\": 0.1284829137724085,\n",
        "                    \"n_estimators\": 796\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 49,\n",
        "                    \"min_child_samples\": 276,\n",
        "                    \"min_child_weight\": 0.008445655331234862,\n",
        "                    \"subsample\": 0.9760533769831113,\n",
        "                    \"colsample_bytree\": 0.989465534702127,\n",
        "                    \"reg_alpha\": 0.5678419494749314,\n",
        "                    \"reg_lambda\": 0.6107277206887869,\n",
        "                    \"learning_rate\": 0.10712275071724532,\n",
        "                    \"n_estimators\": 787\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 47,\n",
        "                    \"min_child_samples\": 70,\n",
        "                    \"min_child_weight\": 0.013349630192554331,\n",
        "                    \"subsample\": 0.8446612641953124,\n",
        "                    \"colsample_bytree\": 0.6028265220878869,\n",
        "                    \"reg_alpha\": 0.046124850082831514,\n",
        "                    \"reg_lambda\": 1.0495493205167783,\n",
        "                    \"learning_rate\": 0.08997219434305109,\n",
        "                    \"n_estimators\": 800\n",
        "                }\n",
        "            ],\n",
        "            \"booster_weight\": [\n",
        "                0.25,\n",
        "                0.25,\n",
        "                0.25,\n",
        "                0.25,\n",
        "            ],\n",
        "            \"features\": features_con\n",
        "        },\n",
        "        \"production\": {\n",
        "            \"data\": df_production,\n",
        "            \"models\": [],\n",
        "            \"mae\": 0,\n",
        "            # Same as for consumption\n",
        "            \"params\": [\n",
        "                {\n",
        "                    \"num_leaves\": 43,\n",
        "                    \"min_child_samples\": 166,\n",
        "                    \"min_child_weight\": 0.013022300234864177,\n",
        "                    \"subsample\": 0.8832290311184181,\n",
        "                    \"colsample_bytree\": 0.608233797718321,\n",
        "                    \"reg_alpha\": 1.9398197043239886,\n",
        "                    \"reg_lambda\": 1.6648852816008435,\n",
        "                    \"learning_rate\": 0.052467822135655234,\n",
        "                    \"n_estimators\": 558\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 47,\n",
        "                    \"min_child_samples\": 70,\n",
        "                    \"min_child_weight\": 0.013349630192554331,\n",
        "                    \"subsample\": 0.8446612641953124,\n",
        "                    \"colsample_bytree\": 0.6028265220878869,\n",
        "                    \"reg_alpha\": 0.046124850082831514,\n",
        "                    \"reg_lambda\": 1.0495493205167783,\n",
        "                    \"learning_rate\": 0.08997219434305109,\n",
        "                    \"n_estimators\": 442\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 41,\n",
        "                    \"min_child_samples\": 107,\n",
        "                    \"min_child_weight\": 0.018208092366233507,\n",
        "                    \"subsample\": 0.7001005442063456,\n",
        "                    \"colsample_bytree\": 0.6155338937717693,\n",
        "                    \"reg_alpha\": 0.6065310293464456,\n",
        "                    \"reg_lambda\": 1.0741648543933109,\n",
        "                    \"learning_rate\": 0.07533024835920818,\n",
        "                    \"n_estimators\": 510\n",
        "                },\n",
        "                {\n",
        "                    \"num_leaves\": 46,\n",
        "                    \"min_child_samples\": 167,\n",
        "                    \"min_child_weight\": 0.005375284391461405,\n",
        "                    \"subsample\": 0.8232408008069365,\n",
        "                    \"colsample_bytree\": 0.7615344684232164,\n",
        "                    \"reg_alpha\": 0.12978449421796312,\n",
        "                    \"reg_lambda\": 0.5078308278686894,\n",
        "                    \"learning_rate\": 0.05937521256772024,\n",
        "                    \"n_estimators\": 487\n",
        "                }\n",
        "            ],\n",
        "            \"booster_weight\": [\n",
        "                0.25,\n",
        "                0.25,\n",
        "                0.25,\n",
        "                0.25,\n",
        "            ],\n",
        "            \"features\": features_prod\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for model_name, model_info in models.items():\n",
        "        df = model_info['data']\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#         with pd.option_context('display.max_columns', None):\n",
        "#             display(df[['row_id', 'datetime', *features]])\n",
        "\n",
        "\n",
        "#         df_final = df[df['datetime'] <= '2023-01-31']\n",
        "#         df_test = df[df['datetime'] > '2023-01-31']\n",
        "#         df_final = df[df['datetime'].dt.month.isin([6, 7, 8, 9, 10])]\n",
        "#         df_test = df[df['datetime'].dt.month.isin([6, 7, 8, 9, 10])]\n",
        "        df_final = df\n",
        "        df_test = df\n",
        "\n",
        "\n",
        "        final_pred = 0\n",
        "        num_boosters = len(model_info['params'])\n",
        "        for i in range(num_boosters):\n",
        "            lgb_dataset = lgb.Dataset(df_final[model_info[\"features\"]], df_final['target'])\n",
        "            params = {\n",
        "                'objective': 'regression',\n",
        "                'metric': 'mae',\n",
        "                'verbose': -1\n",
        "            }\n",
        "            params.update(model_info['params'][i])\n",
        "            model_info['models'].append(lgb.train(params, lgb_dataset))\n",
        "            pred = model_info['models'][i].predict(df_test[model_info[\"features\"]], num_iteration=model_info['models'][i].best_iteration)\n",
        "\n",
        "            final_pred = model_info['booster_weight'][i] * pred + final_pred\n",
        "\n",
        "        mae = mean_absolute_error(df_test['target'], final_pred)\n",
        "        display(Markdown(f'# THE MAE FOR ACTUAL MODEL IS: {mae}'))\n",
        "        display(df['row_id'])\n",
        "        display(final_pred)\n",
        "\n",
        "    return models['production'], models['consumption']\n",
        "\n",
        "model_production, model_consumption = train()"
      ],
      "metadata": {
        "id": "kojEaaisTNxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2d77104-0eb9-479a-8023-8b143c875009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# THE MAE FOR ACTUAL MODEL IS: 31.43669408189782"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0           20497\n",
              "1           20499\n",
              "2           20501\n",
              "3           20503\n",
              "4           20505\n",
              "           ...   \n",
              "992329    2018343\n",
              "992330    2018345\n",
              "992331    2018347\n",
              "992332    2018349\n",
              "992333    2018351\n",
              "Name: row_id, Length: 992334, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992329</th>\n",
              "      <td>2018343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992330</th>\n",
              "      <td>2018345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992331</th>\n",
              "      <td>2018347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992332</th>\n",
              "      <td>2018349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992333</th>\n",
              "      <td>2018351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>992334 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([106.58613594,  19.16398855, 720.9798309 , ..., 308.78611598,\n",
              "        36.61500232, 185.33841987])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/usr/local/lib/python3.11/dist-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# THE MAE FOR ACTUAL MODEL IS: 18.856702120500177"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0           20496\n",
              "1           20498\n",
              "2           20500\n",
              "3           20502\n",
              "4           20504\n",
              "           ...   \n",
              "992329    2018342\n",
              "992330    2018344\n",
              "992331    2018346\n",
              "992332    2018348\n",
              "992333    2018350\n",
              "Name: row_id, Length: 992334, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992329</th>\n",
              "      <td>2018342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992330</th>\n",
              "      <td>2018344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992331</th>\n",
              "      <td>2018346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992332</th>\n",
              "      <td>2018348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992333</th>\n",
              "      <td>2018350</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>992334 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([-1.59926169, -2.22795783,  0.29914659, ..., -1.12334802,\n",
              "       -1.04011274,  1.27806294])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the models\n",
        "Now we will save the models we obtained so we can later re-use them easely"
      ],
      "metadata": {
        "id": "RyjL8obwmotj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, m in enumerate(model_production['models']):\n",
        "    m.save_model(f\"{DRIVE_PATH}/models/kaggle_best_submission/improved/model_production_{i}.txt\")\n",
        "\n",
        "# Write obtained mae to textfile\n",
        "with open(f'{DRIVE_PATH}/models/kaggle_best_submission/improved/mae_production.txt', 'w') as f:\n",
        "    f.write(str(model_production['mae']))\n",
        "\n",
        "for i, m in enumerate(model_consumption['models']):\n",
        "    m.save_model(f\"{DRIVE_PATH}/models/kaggle_best_submission/improved/model_consumption_{i}.txt\")\n",
        "\n",
        "with open(f'{DRIVE_PATH}/models/kaggle_best_submission/improved/mae_consumption.txt', 'w') as f:\n",
        "    f.write(str(model_consumption['mae']))\n"
      ],
      "metadata": {
        "id": "o1WMAjG4nBiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}